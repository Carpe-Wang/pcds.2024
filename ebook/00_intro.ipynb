{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a90a14e-6f10-496b-b8f5-d4c3776d12a9",
   "metadata": {},
   "source": [
    "\n",
    "### Motivation for this Class\n",
    "\n",
    "* Modern computer hardware is parallel at all scales:\n",
    "  * Instruction-level parallelism -- individual processor cores exceute 10s to 100s of instructions at the same time\n",
    "      * processor pipelines execute single instructions over many cycles\n",
    "      * vector computing executes the same instruction on a vector of data\n",
    "  * Multi-core parallelism -- single 'processors' consist of many indpendendent cores\n",
    "  * Multi-processor (Non-Uniform Memory Architecture) parallelism -- many chips intergrated into same computer\n",
    "  * Distributed parallelism -- many computers connected over a network\n",
    "      * Cloud computing\n",
    "      * Supercomputing\n",
    "* Why do we need to use the parallelism? Good utilization leads to:\n",
    "    * Energy efficiency--operating energy of a system constant (to a first approximation). More FLOPSs per watt\n",
    "    * Cost efficiency--fixed cost to acquire hardware. More FLOPs per \\$\\$.\n",
    "    * Scalability--limits to how much hardware can be integrated efficiently.  Solve bigger problems.\n",
    "        * number of cores on a die\n",
    "        * number of processors on a system bus\n",
    "        * number of nodes on a network.\n",
    "    * So, save the earth, make more money, solve the hardest problems.\n",
    "        * Parallel computing is the technology that unlocked AI and started the AI revolution in 2013.\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1165a1-fe91-4624-94f0-789aeed6f917",
   "metadata": {},
   "source": [
    "### A Comment on the CS Curriculum\n",
    "\n",
    "The  computer science curriculum often fails to adequately address parallelism. \n",
    "\n",
    "* The 'discipline' of CS has been built on serial algorithm and machine models\n",
    "    * Computational complexity counts serial instructions\n",
    "    * (Most) Programming languages express one instruction after another\n",
    "    * Systems and archiecture build on the Von Neumann machine.\n",
    "* These are powerful concepts and what you have learned so far.\n",
    "* They are inadequate (at best) and deceptive.\n",
    "\n",
    "* Von Neumann Architecture\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/Von_Neumann_Architecture.svg/2880px-Von_Neumann_Architecture.svg.png\" width=\"386\" title=\"Von Neumann Architecture\" />    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04189fe-2b75-48bb-8b76-73c79d812e12",
   "metadata": {},
   "source": [
    "## Modern Processors: A Programmer's Perspective\n",
    "\n",
    "In 1979, processors looked like Von Neumann's machines.  The were designed for serial execution on a single thread.\n",
    "\n",
    "<img src=\"https://static.righto.com/images/8086-prefetch8088/die-labeled-w600.jpg\" width=\"386\" title=\"8088 Die Layout\" /> \n",
    "\n",
    "For many reasons that we will cover later (Moore's Law and Dennard scaling) processors have evolved into parallel execution units. The goal is always to execute more instructions and this is accomplished in three main ways:\n",
    "\n",
    "\n",
    "### Multicore\n",
    "\n",
    "Processors consist of mulitple independent processing units. Each \"core\" is a separate processing unit.\n",
    "\n",
    "<img src=\"https://www.cpushack.com/wp-content/uploads/2018/03/SB-EPLayout.jpg\" width=\"386\" title=\"Sandy Bridge Die Layout\" /> \n",
    "\n",
    "The process of placing multiple cores on a single die started in 2014 and has continues.  By 2010 (above), 8 cores on a processor was possible. Core counts vary from 4 or 5 (phones) to 96 (servers).\n",
    "\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "The execution of a single instruction is decomposed into stages that use different parts of the chips at the same time. This is known a \"pipeline parallelism\".\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Pipeline%2C_4_stage.svg/1920px-Pipeline%2C_4_stage.svg.png\" width=\"386\" title=\"Generic pipeline (wikipedia)\" /> \n",
    "\n",
    "The instructions need to be independent so that they can run at the same time. If they are not independent, a pipeline stall occurs. This happens when an instruction uses as input the output of a prior instruction.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Pipeline%2C_4_stage_with_bubble.svg/700px-Pipeline%2C_4_stage_with_bubble.svg.png\" width=\"386\" title=\"Generic pipeline (wikipedia)\" /> \n",
    "\n",
    "### Vector Processing\n",
    "\n",
    "Vector processing performs simulatneous instructions on a one-dimensional array (vector) of data. A common implementation is the  _Single Instruction Stream, Multiple Data Stream (SIMD)_ vectors of fixed width. \n",
    "\n",
    "![Vector Operation](./images/vector_op.JPG \"Vector Operation\")\n",
    "\n",
    "### How many parallel operations?\n",
    "\n",
    "At any single time, a CPU can conduct\n",
    "\n",
    "    vector_width x pipeline_depth x core_count\n",
    "    \n",
    "instructions at the same time. Typically 200-1000. \n",
    "\n",
    "Our job, as programmers, is to feed the processor with enough independent work to fully utilize this hardware. Again, for power, cost, carbon. This can be done:\n",
    "\n",
    "* __implicitly__: by feeding the compiler code patterns that result in pipelines without stalls and vectorized code\n",
    "  \n",
    "* __explicitly__\n",
    "    * multithreaded programs (OpenMP, Cilk) that user multiple cores\n",
    "    * vector intrinsics: instructions that program array operations\n",
    "    \n",
    "We will consider both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0540a-2e43-4ee5-ab92-a91cdcdb3605",
   "metadata": {},
   "source": [
    "### Who should take this course?\n",
    " \n",
    "This class aims to provide students with a comprehensive understanding of parallel computing principles, techniques, and best practices. By equipping students with the necessary knowledge and skills, the class seeks to empower them to leverage parallelism effectively and efficiently.\n",
    "\n",
    "This course is designed for the following audiences:\n",
    "\n",
    "* Undergraduates in Computer Science: The course offers a quick lift of skills that are highly valuable to employers and can enhance internship prospects. By taking this course, students can acquire practical parallel computing skills that are in demand in various industries.\n",
    "\n",
    "* Graduate students in Science and Engineering: The course is designed to minimize dependencies on other computer science courses, making it accessible to students from diverse academic backgrounds. The course provides a self-contained treatment of operating systems, computer architecture, and other relevant topics.\n",
    " \n",
    "The course takes an engineering and programming approach, focusing on practical applications rather than delving deeply into the theoretical aspects of parallel computation. This approach makes the course accessible and beneficial to individuals who are interested in understanding how programming languages interact with hardware architecture, particularly the memory system.\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
